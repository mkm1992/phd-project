# -*- coding: utf-8 -*-
"""URLLC_ql.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M5x-5417rCGvwkUTL36fukJWHtdqHC_q
"""

import numpy as np
import random
from IPython.display import clear_output
from collections import deque
import gym
from gym import Env
from gym.spaces import Discrete, Box
import numpy as np
import random


class URLLC(Env):
    def __init__(self):
        self.delay_max = 5.7e-3
        self.UE_Num = 1 #random.randint(1,10)
        self.lambda_vec = random.randint(1,5)*(10^5)
        self.mu = 12*(10^5)
        self.M_max = 100
        self.m_VNF = self.M_max
        self.action_space = Discrete(self.M_max)
        self.observation_space = Box(low=np.array([0,0]), high=np.array([self.M_max,self.delay_max]))
        self.state = [self.M_max, self.delay_max]
    def state_find(self):
        self.delay()
        self.state =  self.m_VNF
        return  self.state

    def delay(self):
        self.delay_mean = 1/(self.mu - self.lambda_vec/self.m_VNF) 
        

    def action_set(self):
        self.VNF_set = np.arange(self.M_max+1)
        return self.VNF_set

    def step(self, action):
        self.m_VNF = action + 1
        self.state_find()
        self.reward_find()
        self.done_find()
        #print(self.delay_mean)
        return self.state, self.reward, self.done, {}
    def done_find(self):
        if self.delay_mean < self.delay_max and self.m_VNF < self.M_max :
          self.done = True
        else:
          self.done = False
          

    def reset(self):
        self.m_VNF = self.M_max
        self.delay_mean = self.delay_max
        self.lambda_vec = random.randint(1,5)*(10^5)
        self.state = [self.m_VNF, self.delay_max]
        return  self.state

    def reward_find(self):
        self.delay_mean = 1/(self.mu - self.lambda_vec/self.m_VNF) 
        if self.delay_mean <= self.delay_max and self.m_VNF <= self.M_max  :
            self.reward = 1/self.delay_mean
        elif self.delay_mean <= self.delay_max and self.m_VNF > self.M_max:
            self.reward = -1/self.delay_mean 
        elif self.delay_mean > self.delay_max:
            self.reward = -1

env = URLLC()

epsilon = 1.0           #Greed 100%
epsilon_min = 0.005     #Minimum greed 0.05%
epsilon_decay = 0.99 #Decay multiplied with epsilon after each episode 
episodes = 25000      #Amount of games
max_steps = 10         #Maximum steps per episode 
learning_rate = 0.5
gamma = 0.65

## 

#env1.available_state() 
env.reset()
Q = np.zeros((101, env.action_space.n))
statePath = np.zeros((episodes, max_steps))    
statePath1 = np.zeros((episodes))   
for episode in range(episodes):
 
    state = 10 #Gets current game state
    done = False        #decides whether the game is over
    score = 0
 
    for k in range(max_steps):
        statePath[episode, k] = state
        # With the probability of (1 - epsilon) take the best action in our Q-table
        
        
        if episode > 18000: #np.random.uniform(0, 1) > epsilon: #episode > 1800: 
        #if np.random.uniform(0, 1) > epsilon:# or episode > 18000:
            action = np.argmax(Q[state, :])
        # Else take a random action
        else:
            action = env.action_space.sample()

        # Step the game forward
        n_state, reward, done, inf = env.step(action)
#        if next_state>26:
#            print('hi')

        # Add up the score
        score += reward
 
        # Update our Q-table with our Q-function
        Q[state, action] = (1 - learning_rate) * Q[state, action] \
            + learning_rate * (reward + gamma * np.max(Q[n_state,:]))
 
        # Set the next state as the current state
        state = n_state
        if done:
            statePath1[episode]= state
            break

        
    # Reducing our epsilon each episode (Exploration-Exploitation trade-off)
    if epsilon >= epsilon_min:
        epsilon *= epsilon_decay


import matplotlib.pyplot as plt  
plt.plot(statePath1)