# -*- coding: utf-8 -*-
"""claasification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gl_81Srjt6_Ml7f5Owz7UvtoTXy5jxKW
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import KFold
from sklearn import svm  
from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import GaussianNB
from scipy.stats.stats import pearsonr
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc, roc_curve
from sklearn.tree import DecisionTreeClassifier, export_graphviz


df = pd.read_csv (r'task.csv')

print(df.shape)
print(df.isnull().values.any())

def visuvalize_data(df):
	X = df['city'].values[:]  # we only take the first two features.
	y = df['view_count'].values[:]
	plt.scatter(X, y)
	plt.xlabel('city')
	plt.ylabel('view count')
	plt.title('city vs view')
	plt.show()
 
visuvalize_data(df)

def visuvalize_data(df):
	X = df['brand'].values[:]  # we only take the first two features.
	y = df['view_count'].values[:]
	plt.scatter(X, y)
	plt.xlabel('brand')
	plt.ylabel('view count')
	plt.title('brand vs view count')
	plt.show()
 
visuvalize_data(df)

df.hist()

size_pd = np.zeros(df.shape)
#df['brand'].values[0]    df[1:2]['brand']
feature_pd = np.zeros([df.shape[1],3])
feature_pd[:,1] = df.var()
feature_pd[:,0] = df.mean()
feature_pd[:,2] = df.median()
print(df.head())

print(df.isnull().sum())

print(df.columns)

df.describe()

####################### first method : change nan to median or mean or padding, also I have drop some feature and test the result but now it is comment

#df['archive_day'].fillna(method='pad', inplace=True)
#df['archive_hour'].fillna(method='pad', inplace=True)
df['archive_day'].fillna(feature_pd[4,0], inplace=True)
df['archive_hour'].fillna(feature_pd[5,0], inplace=True)
#df =df.drop(columns=['archive_hour','archive_day'])
#df = df.dropna()

#df_norm = (df - df.mean()) / (df.max() - df.min())
df1 = df.copy()

plt.figure()
pd.Series(df1['result']).value_counts().sort_index().plot(kind = 'bar')
plt.ylabel("Count")
plt.xlabel("class")
plt.title('0 or 1');

############################################

import seaborn as sns
df1 =df.drop(columns=['view_count', 'get_contact_count'])
df_div = pd.melt(df1, "result", var_name="Characteristics")
fig, ax = plt.subplots(figsize=(10,5))
p = sns.violinplot(ax = ax, x="Characteristics", y="value", hue="result", split = True, data=df_div, inner = 'quartile', palette = 'Set1')
df_no_class = df1.drop(["result"],axis = 1)
p.set_xticklabels(rotation = 90, labels = list(df_no_class.columns));

df_div = pd.melt(df, "result", var_name="Characteristics")
fig, ax = plt.subplots(figsize=(10,5))
p = sns.violinplot(ax = ax, x="Characteristics", y="value", hue="result", split = True, data=df_div, inner = 'quartile', palette = 'Set1')
df_no_class = df1.drop(["result"],axis = 1)
p.set_xticklabels(rotation = 90, labels = list(df_no_class.columns));

plt.figure(figsize=(14,12))
sns.heatmap(df.corr(),linewidths=.1,cmap="YlGnBu", annot=True)
plt.yticks(rotation=0);
#  brand is so important for us since has least corrolation with result

#df1[['result', 'archive_day']].groupby(['archive_day'], as_index=False).mean().sort_values(by='result', ascending=False)

df[['result', 'brand']].groupby(['brand'], as_index=False).mean().sort_values(by='result', ascending=False)

new_var=df[['result', 'brand']]
sns.factorplot('result', col='brand', data=new_var, kind='count', size=2.5, aspect=.8, col_wrap=4);

from sklearn.utils import resample

df_majority = df[df.result==0]
df_minority = df[df.result==1]
 
# Upsample minority class
df_minority_upsampled = resample(df_minority, 
                                 replace=True,     # sample with replacement
                                 n_samples=1330,    # to match majority class
                                 random_state=123) # reproducible results
 
# Combine majority class with upsampled minority class
df_upsampled = pd.concat([df_majority, df_minority_upsampled])
 
# Display new class counts
df_upsampled.result.value_counts()

X=df_upsampled.drop(['result'], axis=1)
Y=df_upsampled['result']
X_train, X_test,Y_train,Y_test = train_test_split(X,Y, test_size = 0.1)

clf = DecisionTreeClassifier()
clf = clf.fit(X_train, Y_train)



features_list = X.columns.values
feature_importance = clf.feature_importances_
sorted_idx = np.argsort(feature_importance)

plt.figure(figsize=(5,7))
plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')
plt.yticks(range(len(sorted_idx)), features_list[sorted_idx])
plt.xlabel('Importance')
plt.title('Feature importances')
plt.draw()
plt.show()

y_pred=clf.predict(X_test)
print("Decision Tree Classifier report \n", classification_report(Y_test, y_pred))

print(accuracy_score(y_pred,Y_test))

cfm=confusion_matrix(Y_test, y_pred)

sns.heatmap(cfm, annot = True,  linewidths=.5, cbar =None)
plt.title('Decision Tree Classifier confusion matrix')
plt.ylabel('True label')
plt.xlabel('Predicted label');

kf = KFold(n_splits = 10, shuffle = True, random_state = 11)
result = next(kf.split(df_upsampled), None)
#print (result)
#train = df.iloc[result[0]]
#test =  df.iloc[result[1]]

C =1
nn =9
rbf_svc = svm.SVC(kernel='rbf', gamma=0.1, C=C)
scores = []
kfold = KFold(n_splits=10, shuffle=True, random_state=42)
for i, (train, test) in enumerate(kfold.split(df)):
  train1 = df_upsampled.iloc[train]
  test1 =  df_upsampled.iloc[test]
  y_train = train1.iloc[:,nn]  
  X_train = train1.iloc[:,:nn] 
  X_test = test1.iloc[:,:nn] 
  y_test =test1.iloc[:,nn] 
  rbf_svc.fit(X_train, y_train)
  score = rbf_svc.score(X_test, y_test)
  scores.append(score)
print(scores)

import statistics
print(statistics.mean(scores))